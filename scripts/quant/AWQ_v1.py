# -*- encoding: utf-8 -*-
# Author  : haitong
# Time    : 2025-05-29 22:59
# File    : AWQ_v1.py
# Software: PyCharm

import random
from datasets import load_dataset
from transformers import AutoTokenizer
from awq import AutoAWQForCausalLM

# ============================== åŸºæœ¬é…ç½® ==============================

# âš ï¸ AWQ å½“å‰ä»…æ”¯æŒ 4-bit æƒé‡é‡åŒ–
int_bits = 4
assert int_bits == 4, "âŒ AWQ ç›®å‰ä»…æ”¯æŒ int_bits = 4"

# æ¨¡å‹è·¯å¾„
model_id = "./models/Llama-3.2-3B-Instruct"
save_path = f"./Int{int_bits}_awq_v1"

# ============================== åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ ==============================

print("âœ… åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ä¸­...")
model = AutoAWQForCausalLM.from_pretrained(
    model_id,
    low_cpu_mem_usage=True,
    use_cache=False
)
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# ============================== å‡†å¤‡æ ¡å‡†æ•°æ®é›† ==============================

print("âœ… å‡†å¤‡æ ¡å‡†æ•°æ®é›†ï¼šGSM8K + ARC-Challenge")

# ---- GSM8K ----
gsm_dataset = load_dataset("gsm8k", "main", split="train")
gsm_samples = random.sample(list(gsm_dataset), 100)
gsm_texts = [f"Question: {item['question'].strip()}\nAnswer: " for item in gsm_samples]
print(f"  âœ… GSM8K æ ·æœ¬æ•°: {len(gsm_texts)}")

# ---- ARC-Challenge ----
arc_train = load_dataset("ai2_arc", "ARC-Challenge", split="train")
arc_val = load_dataset("ai2_arc", "ARC-Challenge", split="validation")
arc_samples = random.sample(list(arc_train), 80) + random.sample(list(arc_val), 20)

arc_texts = []
for item in arc_samples:
    q = f"Question: {item['question'].strip()}\n"
    for idx, choice in enumerate(item["choices"]["text"]):
        q += f"({chr(65 + idx)}) {choice.strip()}\n"
    q += "Answer: "
    arc_texts.append(q)
print(f"  âœ… ARC-Challenge æ ·æœ¬æ•°: {len(arc_texts)}")

# åˆå¹¶ + æ‰“ä¹±
calibration_texts = gsm_texts + arc_texts
random.shuffle(calibration_texts)
print(f"  âœ… åˆå¹¶åå…±è®¡: {len(calibration_texts)} æ¡æ ¡å‡†æ ·æœ¬")

# ============================== è®¾ç½®é‡åŒ–é…ç½® ==============================

print("âœ… è®¾ç½® AWQ é‡åŒ–é…ç½®")
quant_config = {
    "zero_point": True,
    "q_group_size": 64,
    "w_bit": int_bits,
    "version": "GEMM"  # ä¹Ÿå¯ä½¿ç”¨ TURBOï¼Œä½†éœ€è‡ªè¡Œç¼–è¯‘
}

# ============================== æ‰§è¡Œ AWQ é‡åŒ– ==============================

print(f"âœ… å¼€å§‹æ‰§è¡Œ INT{int_bits} é‡åŒ–...")
model.quantize(
    tokenizer=tokenizer,
    quant_config=quant_config,
    calib_data=calibration_texts,
    n_parallel_calib_samples=64
)

# ============================== ä¿å­˜é‡åŒ–æ¨¡å‹ ==============================

print(f"âœ… ä¿å­˜ INT{int_bits} é‡åŒ–æ¨¡å‹è‡³: {save_path}")
model.save_quantized(save_path, safetensors=True)
tokenizer.save_pretrained(save_path)

print(f"ğŸ‰ INT{int_bits} é‡åŒ–å®Œæˆï¼Œæ¨¡å‹ä¿å­˜æˆåŠŸï¼")
