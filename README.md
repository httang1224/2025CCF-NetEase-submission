# é¢å‘å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„å°å‹åŒ–éƒ¨ç½²ä¼˜åŒ–æ–¹æ³•ç ”ç©¶

æœ¬é¡¹ç›®è‡´åŠ›äºç ”ç©¶å¦‚ä½•åœ¨ä¿è¯ç²¾åº¦çš„å‰æä¸‹ï¼Œå®ç°å¤§æ¨¡å‹åœ¨æœ‰é™ç®—åŠ›å¹³å°ä¸Šçš„é«˜æ•ˆéƒ¨ç½²ã€‚æˆ‘ä»¬ä»¥ Llama-3.2-3B-Instruct ä¸ºåŸºå‡†æ¨¡å‹ï¼Œé‡‡ç”¨ GPTQå’ŒAWQ è¿›è¡Œé‡åŒ–ä¼˜åŒ–ï¼Œé…åˆ `vLLM` å’Œ `lm-evaluation-harness` è¿›è¡Œç²¾åº¦ä¸å»¶è¿Ÿè¯„ä¼°ã€‚

---

## ğŸ§© é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ benchmarks/                  # æ¨ç†å»¶è¿Ÿè¯„ä¼°è„šæœ¬
â”‚   â”œâ”€â”€ benchmark_latency.py
â”‚   â””â”€â”€ benchmark_utils.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ Llama-3.2-3B-Instruct/   # åŸå§‹/ä¼˜åŒ–åæ¨¡å‹è·¯å¾„
â”œâ”€â”€ outputs/                     # ç²¾åº¦ä¸å»¶è¿Ÿè¯„ä¼°ç»“æœ
â”‚   â”œâ”€â”€ acc.json
â”‚   â””â”€â”€ perf.json
â””â”€â”€ ...
```

---

## ğŸ›  ç¯å¢ƒåˆ›å»º

```bash
git clone https://github.com/httang1224/2025CCF-NetEase-submission.git
git clone git@github.com:httang1224/2025CCF-NetEase-submission.git

cd 2025CCF-NetEase-submission

conda create -n llm_compress python=3.9 -y
conda activate llm_compress

pip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu121
pip install auto-gptq==0.7.1
pip install datasets==2.17.0
pip install vllm==0.7.1
pip install lm-eval==0.4.8
```

---

## ğŸ“¦ æ¨¡å‹å‡†å¤‡

```bash
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download --token <your_token> --resume-download \
  meta-llama/Llama-3.2-3B-Instruct \
  --local-dir meta-llama/Llama-3.2-3B-Instruct
```

---

## ğŸ“Š åŸå§‹æ¨¡å‹æ€§èƒ½åŸºå‡†

### â± æ¨ç†å»¶è¿Ÿæµ‹è¯•

```bash
python3 ./benchmarks/benchmark_latency.py \
  --model ./meta-llama/Llama-3.2-3B-Instruct/ \
  --input-len 4096 --output-len 100 --batch-size 1
```

ç¤ºä¾‹ç»“æœï¼š

```
TTFT: 0.2929 s
TPOT: 0.0151 s
weights_memory: 6127.83 MB
```

### âœ… ç²¾åº¦è¯„ä¼°

#### GSM8K

```bash
lm-eval --model vllm --model_args pretrained=./meta-llama/Llama-3.2-3B-Instruct/,gpu_memory_utilization=0.6,dtype=auto \
  --tasks gsm8k --batch_size auto:1 --output_path ./outputs/
```

ç²¾åº¦ï¼ˆ5-shotï¼‰ï¼š

```
exact_match: 0.6543 Â± 0.0131
```

#### ARC-Challenge

```bash
lm-eval --model vllm --model_args pretrained=./meta-llama/Llama-3.2-3B-Instruct/,gpu_memory_utilization=0.6,dtype=auto \
  --tasks arc_challenge --batch_size auto:1 --output_path ./outputs/
```

ç²¾åº¦ï¼ˆ0-shotï¼‰ï¼š

```
acc_norm: 0.4582 Â± 0.0146
```

---


## ğŸ§ª é‡åŒ–åæ¨¡å‹è¯„ä¼°

è¯¦è§ `outputs/acc.json` ä¸ `outputs/perf.json` æ–‡ä»¶ï¼Œæˆ–è¿è¡Œä»¥ä¸‹è„šæœ¬ï¼š

```bash
bash run_perf.sh   # è¿è¡Œæ€§èƒ½è¯„ä¼°
bash run_acc.sh    # è¿è¡Œç²¾åº¦è¯„ä¼°
```

---


## ğŸ§® æ¯”èµ›è¯„ä»·æŒ‡æ ‡è¯´æ˜

### 1. ç²¾åº¦èƒ½åŠ›è¯„ä¼°ï¼ˆAccuracyï¼‰

ä»¥ ARC_challenge ä¸ GSM8K ä¸¤ä¸ªä»»åŠ¡åœ¨ `lm-eval` ä¸Šçš„å¹³å‡åˆ†ä½œä¸ºè¡¡é‡æ ‡å‡†ã€‚

- å‚è€ƒæŒ‡æ ‡ = Llama-3.2-3B-Instruct ç²¾åº¦å‡å€¼
- æäº¤æ¨¡å‹æŒ‡æ ‡ = ä½ ä¼˜åŒ–åçš„æ¨¡å‹ç²¾åº¦å‡å€¼

### 2. æ€§èƒ½æŒ‡æ ‡è¯„ä¼°ï¼ˆEfficiencyï¼‰

- **æ¨¡å‹å‹ç¼©ç‡**ï¼šæ¨¡å‹æƒé‡å¤§å°æ¯”å€¼
- **æ¨ç†é€Ÿåº¦æå‡**ï¼š
  - TTFTï¼ˆTime to First Tokenï¼‰
  - TPOTï¼ˆTime Per Output Tokenï¼‰

å‡ä¸åŸå§‹æ¨¡å‹å¯¹æ¯”ï¼Œè®¡ç®—æå‡ç‡ã€‚

### 3. æ€»åˆ†è®¡ç®—

æ‰€æœ‰ç²¾åº¦ä¸æ€§èƒ½é¡¹å‡æŒ‰å‚èµ›æœ€é«˜å€¼å½’ä¸€åŒ–ï¼Œè¿›è¡ŒåŠ æƒæ±‚å’Œå¾—å‡ºæœ€ç»ˆå¾—åˆ†ã€‚

---


## ğŸ–¥ï¸ éªŒè¯ç¯å¢ƒè¯´æ˜

> âš ï¸ æ³¨æ„ï¼šè™½ç„¶é¡¹ç›®è¯„ä¼°éœ€æ±‚æ ‡å‡†åŸºäº 4090 GPUï¼Œä½†æœ¬å®éªŒä¸æ€§èƒ½æµ‹è¯•å®åˆ™åœ¨ **NVIDIA A40 (48GB)** ä¸Šå®Œæˆ

- å¹³å°ï¼šç½‘æ˜“ä¸¹ç‚‰äº‘å¹³å°
- GPUï¼šRTX 4090
- CUDAï¼š12.1
- vLLMï¼š0.7.1
- PyTorchï¼š2.5.1
- lm-evalï¼š0.4.8

---

